{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating |datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "# import data\n",
    "from DataExtraction import dataNoMass as data\n",
    "from DataExtraction import dataWithP2\n",
    "from DataExtraction import dataWithP2E2 \n",
    "from DataExtraction import dataWithMass \n",
    "from DataExtraction import p2E2 \n",
    "from DataExtraction import e2P2Dec \n",
    "from DataExtraction import labels2D as labels\n",
    "#from DataExtraction import labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "# labels = np.row_stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# normalize test data\n",
    "# train_data[:,0] = train_data[:,0] / np.linalg.norm(train_data[:,0]) # normalize column 0\n",
    "# train_data[:,1] = train_data[:,1] / np.linalg.norm(train_data[:,1]) # normalize column 1\n",
    "#normalize train data\n",
    "# test_data[:,0] = test_data[:,0] / np.linalg.norm(test_data[:,0]) # normalize column 0\n",
    "# test_data[:,1] = test_data[:,1] / np.linalg.norm(test_data[:,1]) # normalize column 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avgE2 = np.mean(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data/avgE2\n",
    "test_data = test_data/avgE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messing with the number of training data points\n",
    "# train_data = train_data[0:9]\n",
    "# train_labels = train_labels[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# loss functions\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    #loss = -1/m * np.sum(y * np.log(y_hat))\n",
    "    return loss\n",
    "\n",
    "def crossEntropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    if y.all() == 1:\n",
    "        return -1/m * np.sum(np.log(y_hat))\n",
    "    else:\n",
    "        return -1/m * np.sum(np.log(1 - y_hat))\n",
    "\n",
    "def mse_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return np.sum((y_hat - y)**2) / m\n",
    "    \n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# This is the BACKWARD PROPAGATION function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n",
    "\n",
    "#TRAINING PHASE\n",
    "def initialize_parameters(input_dim,l1_dim, l2_dim, output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(input_dim, l1_dim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, l1_dim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(l1_dim, l2_dim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, l2_dim))\n",
    "    W3 = 2 * np.random.rand(l2_dim, output_dim) - 1\n",
    "    b3 = np.zeros((1,output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # load parameters into running lists\n",
    "    w00s.append(W1[0][0]) # modifies global list\n",
    "    w01s.append(W1[0][1]) # modifies global list\n",
    "    w02s.append(W1[0][2]) # modifies global list\n",
    "    w03s.append(W1[0][3]) # modifies global list\n",
    "    w04s.append(W1[0][4]) # modifies global list\n",
    "    \n",
    "    w10s.append(W1[1][0]) # modifies global list\n",
    "    w11s.append(W1[1][1]) # modifies global list\n",
    "    w12s.append(W1[1][2]) # modifies global list\n",
    "    w13s.append(W1[1][3]) # modifies global list\n",
    "    w14s.append(W1[1][4]) # modifies global list\n",
    "    \n",
    "    b0s.append(b1[0][0]) # modifies global list\n",
    "    b1s.append(b1[0][1]) # modifies global list\n",
    "    b2s.append(b1[0][2]) # modifies global list\n",
    "    b3s.append(b1[0][3]) # modifies global list\n",
    "    b4s.append(b1[0][4]) # modifies global list\n",
    "\n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = c['a3']\n",
    "    # plotArr.append([x, y_hat]) #added to make plot\n",
    "    return y_hat\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "def train(model,X_,y_,learning_rate, epochs=2001, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "        # it is at this point in the training that the weights get added to the lists\n",
    "    \n",
    "        a3 = cache['a3']\n",
    "        thisLoss = mse_loss(y_,a3) # set loss function here\n",
    "        losses.append(thisLoss) # modifies global list\n",
    "        y_hat = predict(model,X_) # getting rid of this because it's wrong\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        accur = accuracy_score(a3,train_labels)\n",
    "        train_accuracies.append(accur) # modifies global list\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            placeholderVar = accuracy_score(a3, train_labels)\n",
    "            test_accuracy = accuracyOfModel(model, test_data, test_labels)\n",
    "            test_accuracies.append(test_accuracy) # modifies global list\n",
    "            test_num.append(i)\n",
    "        #Printing loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 300==0:\n",
    "            print('Loss after iteration',i,':',thisLoss)\n",
    "            print('Train Accuracy after iteration',i,':',accur*100,'%')\n",
    "            print('Test Accuracy after iteration',i,':',test_accuracy*100,'%')\n",
    "    return model\n",
    "\n",
    "# TESTING PHASE\n",
    "# test the accuracy of any model\n",
    "def accuracyOfModel(_model, _testData, _testLabels):\n",
    "    y_pred = predict(_model,_testData) # make predictions on test data\n",
    "    y_true = _testLabels # get usable info from labels\n",
    "    return accuracy_score(y_pred, y_true)\n",
    "\n",
    "def accuracy_score(_outputNodes, _labels):\n",
    "    for i in range(len(_outputNodes)-1):\n",
    "        if _outputNodes[i][0]>.5:\n",
    "            _outputNodes[i]=[1,0]\n",
    "        else:\n",
    "            _outputNodes[i]=[0,1]\n",
    "    numWrong = np.count_nonzero(np.subtract(_outputNodes,_labels))/2\n",
    "    return (len(_outputNodes)-numWrong)/len(_outputNodes)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global lists\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "\n",
    "w00s = []\n",
    "w01s = []\n",
    "w02s = []\n",
    "w03s = []\n",
    "w04s = []\n",
    "\n",
    "w10s = []\n",
    "w11s = []\n",
    "w12s = []\n",
    "w13s = []\n",
    "w14s = []\n",
    "\n",
    "b0s = []\n",
    "b1s = []\n",
    "b2s = []\n",
    "b3s = []\n",
    "b4s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 0.5694013148524751\n",
      "Train Accuracy after iteration 0 : 50.22881848752244 %\n",
      "Test Accuracy after iteration 0 : 49.945639806831686 %\n",
      "Loss after iteration 300 : 0.500973834439016\n",
      "Train Accuracy after iteration 300 : 51.96328790675331 %\n",
      "Test Accuracy after iteration 300 : 51.77871608808879 %\n",
      "Loss after iteration 600 : 0.5005654424977339\n",
      "Train Accuracy after iteration 600 : 51.970873049986096 %\n",
      "Test Accuracy after iteration 600 : 51.73573360976966 %\n",
      "Loss after iteration 900 : 0.5002861884406224\n",
      "Train Accuracy after iteration 900 : 51.945589239210136 %\n",
      "Test Accuracy after iteration 900 : 51.738261990847256 %\n",
      "Loss after iteration 1200 : 0.5000823494401277\n",
      "Train Accuracy after iteration 1200 : 51.932947333822156 %\n",
      "Test Accuracy after iteration 1200 : 51.71044979899371 %\n",
      "Loss after iteration 1500 : 0.4999270959159021\n",
      "Train Accuracy after iteration 1500 : 51.953174382442924 %\n",
      "Test Accuracy after iteration 1500 : 51.745847134080044 %\n",
      "Loss after iteration 1800 : 0.4998052154777084\n",
      "Train Accuracy after iteration 1800 : 51.945589239210136 %\n",
      "Test Accuracy after iteration 1800 : 51.743318753002455 %\n",
      "Loss after iteration 2100 : 0.49970732417676905\n",
      "Train Accuracy after iteration 2100 : 51.93800409597734 %\n",
      "Test Accuracy after iteration 2100 : 51.72814846653687 %\n"
     ]
    }
   ],
   "source": [
    "# declare global lists\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_num = []\n",
    "\n",
    "learnRate = 0.005 # set learning rate here\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(4, 6, 6, 2)\n",
    "model = train(model,train_data,train_labels,learning_rate=learnRate,epochs=2101,print_loss=True) # original learning rate is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.scatter(test_num, test_accuracies, label=\"Test Accuracy\", s=16, color=\"green\")\n",
    "#plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.plot()\n",
    "plt.legend()\n",
    "# plt.title(\"Network Loss and Accuracy per Epoch with %1.3f Learning Rate\" %learnRate)\n",
    "plt.title(\"Network Loss and Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.35,1)\n",
    "plt.savefig(\"ptEtaPhiEAcc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot((w00s-(w00s[0])), label=\"Weight 0.0\")\n",
    "plt.plot((w01s-(w01s[0])), label=\"Weight 0.1\")\n",
    "plt.plot((w02s-(w02s[0])), label=\"Weight 0.2\")\n",
    "plt.plot((w03s-(w03s[0])), label=\"Weight 0.3\")\n",
    "plt.plot((w04s-(w04s[0])), label=\"Weight 0.4\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Weights From the First Input Node at Each Epoch\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot((w10s-(w10s[0])), label=\"Weight 1.0\")\n",
    "plt.plot((w11s-(w11s[0])), label=\"Weight 1.1\")\n",
    "plt.plot((w12s-(w12s[0])), label=\"Weight 1.2\")\n",
    "plt.plot((w13s-(w13s[0])), label=\"Weight 1.3\")\n",
    "plt.plot((w14s-(w14s[0])), label=\"Weight 1.4\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Change in Weights From the Second Input Node at Each Epoch\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(b0s, label=\"Bias 0\")\n",
    "plt.plot(b1s, label=\"Bias 1\")\n",
    "plt.plot(b2s, label=\"Bias 2\")\n",
    "plt.plot(b3s, label=\"Bias 3\")\n",
    "plt.plot(b4s, label=\"Bias 4\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Biases from Input Nodes at Each Epoch\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "print(\"Weight 1: \\n\", W1)\n",
    "print(\"Weight 2: \\n\", W2)\n",
    "print(\"Weight 3: \\n\", W3)\n",
    "print(\"Bias 1: \\n\", b1)\n",
    "print(\"Bias 2: \\n\", b2)\n",
    "print(\"Bias 3: \\n\", b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of output node 2 vs output node 1\n",
    "# get weights and biases\n",
    "W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "\n",
    "diffArray = []\n",
    "\n",
    "plotX = []\n",
    "plotY = []\n",
    "\n",
    "inputArr = []\n",
    "outputArr = []\n",
    "for i in range(len(test_data)-1):\n",
    "    _a0 = test_data[i]\n",
    "    diffArray.append(_a0[1]-_a0[0])\n",
    "    inputArr.append(_a0[1])\n",
    "    _z1 = _a0.dot(W1) + b1\n",
    "    # Put it through the first activation function\n",
    "    _a1 = np.tanh(_z1)\n",
    "    # Second linear step\n",
    "    _z2 = _a1.dot(W2) + b2\n",
    "    # Second activation function\n",
    "    _a2 = np.tanh(_z2)\n",
    "    #Third linear step\n",
    "    _z3 = _a2.dot(W3) + b3\n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    _a3 = softmax(_z3)\n",
    "    plotX.append(_a3[0][0])\n",
    "    plotY.append(_a3[0][1])\n",
    "plt.scatter(plotX, plotY)\n",
    "plt.title(\"Output node 2 vs Output node 1\")\n",
    "\n",
    "    # Calculate the point density\n",
    "#     xy = np.vstack([plotX,plotY])\n",
    "#     z = gaussian_kde(xy)(xy)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.scatter(x, y, c=z, s=100, edgecolor='')\n",
    "#     plt.show()\n",
    "\n",
    "#plt.hist(diffArray, bins=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffArr = []\n",
    "discardedVals = 0\n",
    "for x, y in zip(plotX, plotY):\n",
    "    if(abs(y-x)<.75):\n",
    "        diffArr.append(y-x)\n",
    "    else:\n",
    "        discardedVals += 1\n",
    "print(discardedVals, \"differences greater than .75\")\n",
    "plt.hist(diffArr, bins=1000)\n",
    "plt.title(\"Difference in Electron and Muon Output Node Values (E-M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
